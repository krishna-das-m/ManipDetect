{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f45e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install praw neptune-client==1.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007d1db2",
   "metadata": {},
   "source": [
    "### Libraries to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19697dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import neptune\n",
    "import praw\n",
    "from datetime import datetime\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1296a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_progress(filename=\"scraping_progress.json\"):\n",
    "    \"\"\"Load previously scraped data if it exists\"\"\"\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            return data.get('posts', []), data.get('last_post_id', None)\n",
    "    except FileNotFoundError:\n",
    "        return [], None\n",
    "\n",
    "def save_progress(posts_data, last_post_id, filename=\"scraping_progress.json\"):\n",
    "    \"\"\"Save current progress to file\"\"\"\n",
    "    progress_data = {\n",
    "        'posts': posts_data,\n",
    "        'last_post_id': last_post_id,\n",
    "        'saved_at': datetime.now().isoformat(),\n",
    "        'total_posts': len(posts_data)\n",
    "    }\n",
    "    # Save progress as JSON (for resume functionality)\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(progress_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # Also save current data as CSV\n",
    "    if posts_data:\n",
    "        df = pd.DataFrame(posts_data)\n",
    "        df.to_csv(\"wallstreetbets_posts.csv\", index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8cf445da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_final_csv(posts_data):\n",
    "    \"\"\"Save final dataset as CSV with proper formatting\"\"\"\n",
    "    if not posts_data:\n",
    "        return\n",
    "        \n",
    "    df = pd.DataFrame(posts_data)\n",
    "    \n",
    "    # Convert timestamp to readable format\n",
    "    df['created_datetime'] = pd.to_datetime(df['created_utc'], unit='s')\n",
    "    \n",
    "    # Reorder columns for better readability\n",
    "    columns_order = ['post_id', 'title', 'author_name', 'score', 'num_comments', \n",
    "                    'created_utc', 'created_datetime', 'url']\n",
    "    df = df[columns_order]\n",
    "    \n",
    "    # Save with timestamp in filename\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    csv_filename = f\"wallstreetbets_posts_{timestamp}.csv\"\n",
    "    df.to_csv(csv_filename, index=False, encoding='utf-8')\n",
    "    \n",
    "    return csv_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9a4b05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reddit_connect():\n",
    "    \"\"\"Initialize Reddit connection\"\"\"\n",
    "    load_dotenv()\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=os.getenv('REDDIT_CLIENT_ID'),\n",
    "        client_secret=os.getenv('REDDIT_CLIENT_SECRET'),\n",
    "        user_agent=os.getenv('REDDIT_USER_AGENT'),\n",
    "        username=os.getenv('REDDIT_USERNAME'),\n",
    "        password=os.getenv('REDDIT_PASSWORD')\n",
    "    )\n",
    "    return reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2d3f780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(reddit, target_posts=50):\n",
    "    \"\"\"Build the dataset by scraping WallStreetBets posts\"\"\"\n",
    "    \n",
    "    # Load previous progress\n",
    "    posts_data, last_post_id = load_progress()\n",
    "    start_count = len(posts_data)\n",
    "    \n",
    "    if start_count > 0:\n",
    "        print(f\"Resuming from {start_count} previously scraped posts...\")\n",
    "    else:\n",
    "        print(\"Starting fresh scrape...\")\n",
    "    \n",
    "    # Get subreddit\n",
    "    subreddit = reddit.subreddit(\"wallstreetbets\")\n",
    "    \n",
    "    # Track scraping metrics\n",
    "    start_time = time.time()\n",
    "    errors_count = 0\n",
    "    \n",
    "    try:\n",
    "        # Get posts (PRAW handles pagination automatically)\n",
    "        posts_generator = subreddit.new(limit=target_posts)\n",
    "        \n",
    "        # Convert to list to get total count for progress bar\n",
    "        print(\"Fetching post list from Reddit...\")\n",
    "        all_posts = list(posts_generator)\n",
    "        \n",
    "        # Skip posts we already have if resuming\n",
    "        if last_post_id:\n",
    "            # Find where to resume\n",
    "            resume_index = 0\n",
    "            for i, post in enumerate(all_posts):\n",
    "                if post.id == last_post_id:\n",
    "                    resume_index = i + 1\n",
    "                    break\n",
    "            all_posts = all_posts[resume_index:]\n",
    "            print(f\"Resuming from post index {resume_index}\")\n",
    "        \n",
    "        # Process remaining posts\n",
    "        posts_to_process = min(len(all_posts), target_posts - start_count)\n",
    "        \n",
    "        for i, submission in enumerate(tqdm(all_posts[:posts_to_process], \n",
    "                                        desc=\"Scraping posts\", \n",
    "                                        initial=start_count, \n",
    "                                        total=target_posts)):\n",
    "            try:\n",
    "                # Handle author safely\n",
    "                author_name = \"[deleted]\"\n",
    "                if submission.author is not None:\n",
    "                    try:\n",
    "                        author_name = submission.author.name\n",
    "                    except Exception:\n",
    "                        author_name = \"[unavailable]\"\n",
    "                \n",
    "                # Extract post data (keeping titles and emojis intact)\n",
    "                post_info = {\n",
    "                    \"post_id\": submission.id,\n",
    "                    \"title\": submission.title,  # Preserves emojis and formatting\n",
    "                    \"score\": submission.score,\n",
    "                    \"created_utc\": submission.created_utc,\n",
    "                    \"num_comments\": submission.num_comments,\n",
    "                    \"url\": submission.url,\n",
    "                    \"author_name\": author_name\n",
    "                }\n",
    "                \n",
    "                posts_data.append(post_info)\n",
    "                \n",
    "                # Save progress every 50 posts\n",
    "                if (len(posts_data) - start_count) % 50 == 0:\n",
    "                    save_progress(posts_data, submission.id)\n",
    "                \n",
    "                # Small delay every 100 posts to be nice to Reddit\n",
    "                if (len(posts_data) - start_count) % 100 == 0:\n",
    "                    time.sleep(1)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                errors_count += 1\n",
    "                print(f\"Error processing post {submission.id}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Final save\n",
    "        save_progress(posts_data, posts_data[-1][\"post_id\"] if posts_data else None)\n",
    "        \n",
    "        # Calculate final metrics\n",
    "        end_time = time.time()\n",
    "        scraping_duration = end_time - start_time\n",
    "        \n",
    "        # Save final CSV dataset\n",
    "        csv_filename = save_final_csv(posts_data)\n",
    "        \n",
    "        # Return metrics for tracking\n",
    "        metrics = {\n",
    "            'total_posts_collected': len(posts_data),\n",
    "            'new_posts_this_session': len(posts_data) - start_count,\n",
    "            'scraping_duration_minutes': scraping_duration / 60,\n",
    "            'errors_encountered': errors_count,\n",
    "            'posts_per_minute': (len(posts_data) - start_count) / (scraping_duration / 60) if scraping_duration > 0 else 0,\n",
    "            'csv_filename': csv_filename,\n",
    "            'total_posts_available': len(all_posts),\n",
    "            'resumed_from': start_count if start_count > 0 else None\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nDataset building completed!\")\n",
    "        print(f\"Total posts collected: {metrics['total_posts_collected']}\")\n",
    "        print(f\"New posts this session: {metrics['new_posts_this_session']}\")\n",
    "        print(f\"Duration: {metrics['scraping_duration_minutes']:.2f} minutes\")\n",
    "        print(f\"Errors: {metrics['errors_encountered']}\")\n",
    "        \n",
    "        return posts_data, metrics\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nScraping interrupted by user. Progress saved.\")\n",
    "        save_progress(posts_data, posts_data[-1][\"post_id\"] if posts_data else None)\n",
    "        return posts_data, {'interrupted': True, 'posts_at_interruption': len(posts_data)}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {str(e)}\")\n",
    "        save_progress(posts_data, posts_data[-1][\"post_id\"] if posts_data else None)\n",
    "        return posts_data, {'fatal_error': str(e), 'posts_at_error': len(posts_data)}\n",
    "\n",
    "def track_scraping_metrics(posts_data, metrics, target_posts):\n",
    "    \"\"\"Track scraping metrics and results in Neptune\"\"\"\n",
    "    # Initialize Neptune\n",
    "    run = neptune.init_run(project=\"krishnadasm/wallstreetbets-scraper\")\n",
    "    \n",
    "    # Log configuration\n",
    "    run[\"config/target_posts\"] = target_posts\n",
    "    run[\"config/subreddit\"] = \"wallstreetbets\"\n",
    "    run[\"config/sort_method\"] = \"new\"\n",
    "    run[\"config/resume_enabled\"] = True\n",
    "    \n",
    "    # Log scraping metrics\n",
    "    if 'interrupted' in metrics:\n",
    "        run[\"scraping/interrupted\"] = True\n",
    "        run[\"scraping/posts_at_interruption\"] = metrics['posts_at_interruption']\n",
    "    elif 'fatal_error' in metrics:\n",
    "        run[\"scraping/fatal_error\"] = metrics['fatal_error']\n",
    "        run[\"scraping/posts_at_error\"] = metrics['posts_at_error']\n",
    "    else:\n",
    "        # Log successful completion metrics\n",
    "        run[\"results/total_posts_collected\"] = metrics['total_posts_collected']\n",
    "        run[\"results/new_posts_this_session\"] = metrics['new_posts_this_session']\n",
    "        run[\"results/scraping_duration_minutes\"] = metrics['scraping_duration_minutes']\n",
    "        run[\"results/errors_encountered\"] = metrics['errors_encountered']\n",
    "        run[\"results/posts_per_minute\"] = metrics['posts_per_minute']\n",
    "        run[\"scraping/total_posts_available\"] = metrics['total_posts_available']\n",
    "        \n",
    "        if metrics['resumed_from']:\n",
    "            run[\"scraping/resumed_from\"] = metrics['resumed_from']\n",
    "        \n",
    "        # Upload final dataset to Neptune\n",
    "        if metrics.get('csv_filename'):\n",
    "            run[\"data/posts_dataset\"].upload(metrics['csv_filename'])\n",
    "        run[\"data/progress_file\"].upload(\"scraping_progress.json\")\n",
    "    \n",
    "    # Log final progress\n",
    "    run[\"scraping/final_progress\"] = len(posts_data)\n",
    "    \n",
    "    run.stop()\n",
    "    print(\"Metrics logged to Neptune successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "788993d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Reddit API successfully.\n",
      "Starting fresh scrape...\n",
      "Fetching post list from Reddit...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping posts: 100%|██████████| 50/50 [00:00<00:00, 1020.77it/s]\n",
      "c:\\Users\\krishnadas\\anaconda3\\Lib\\site-packages\\neptune\\common\\warnings.py:62: NeptuneWarning: To avoid unintended consumption of logging hours during interactive sessions, the following monitoring options are disabled unless set to 'True' when initializing the run: 'capture_stdout', 'capture_stderr', and 'capture_hardware_metrics'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset building completed!\n",
      "Total posts collected: 50\n",
      "New posts this session: 50\n",
      "Duration: 0.04 minutes\n",
      "Errors: 0\n",
      "https://app.neptune.ai/krishnadasm/wallstreetbets-scraper/e/WAL-5\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 13 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 13 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/krishnadasm/wallstreetbets-scraper/e/WAL-5/metadata\n",
      "Metrics logged to Neptune successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def scrape_wallstreetbets():\n",
    "    \"\"\"Main function that orchestrates the scraping process\"\"\"\n",
    "    # Build the dataset\n",
    "    reddit = reddit_connect()\n",
    "    print(\"Connected to Reddit API successfully.\")\n",
    "    posts_data, metrics = build_dataset(reddit, target_posts=50)\n",
    "    \n",
    "    # Track metrics in Neptune\n",
    "    track_scraping_metrics(posts_data, metrics, target_posts=50)\n",
    "    \n",
    "    return posts_data, metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_wallstreetbets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecae9cd",
   "metadata": {},
   "source": [
    "Environments setup for Reddit and neptune.ai secrets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf2ca58",
   "metadata": {},
   "source": [
    "### Neptune.ai Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bccee61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token loaded: eyJhcGlfYW...\n",
      "https://app.neptune.ai/krishnadasm/wallstreetbets-scraper/e/WAL-4\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 1 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 1 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/krishnadasm/wallstreetbets-scraper/e/WAL-4/metadata\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# Test that it loaded\n",
    "print(\"Token loaded:\", os.getenv('NEPTUNE_API_TOKEN')[:10] + \"...\")  # Only shows first 10 chars\n",
    "\n",
    "# neptune_api_token = user_secrets.get_secret(\"neptune_api\")\n",
    "run = None\n",
    "try:\n",
    "    run = neptune.init_run(\n",
    "    project=\"krishnadasm/wallstreetbets-scraper\"\n",
    "    )\n",
    "    run[\"test\"] = \"Connected with .env file!\"\n",
    "    run.stop()\n",
    "except Exception as ex:\n",
    "    print(f\"Exception: {ex}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97a28f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install python-dotenv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
