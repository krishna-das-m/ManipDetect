{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46f45e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install praw neptune dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007d1db2",
   "metadata": {},
   "source": [
    "### Libraries to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19697dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: neptune 1.14.0\n",
      "Uninstalling neptune-1.14.0:\n",
      "  Successfully uninstalled neptune-1.14.0\n",
      "Found existing installation: neptune-client 1.2.0\n",
      "Uninstalling neptune-client-1.2.0:\n",
      "  Successfully uninstalled neptune-client-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting neptune\n",
      "  Using cached neptune-1.14.0-py3-none-any.whl (487 kB)\n",
      "Requirement already satisfied: Pillow>=1.1.6 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from neptune) (9.2.0)\n",
      "Requirement already satisfied: requests>=2.20.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from neptune) (2.32.3)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from neptune) (1.16.0)\n",
      "Requirement already satisfied: swagger-spec-validator>=2.7.4 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from neptune) (3.0.4)\n",
      "Requirement already satisfied: oauthlib>=2.1.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from neptune) (3.3.1)\n",
      "Requirement already satisfied: GitPython>=2.0.8 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from neptune) (3.1.44)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from neptune) (4.13.2)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from neptune) (8.2.0)\n",
      "Requirement already satisfied: websocket-client!=1.0.0,>=0.35.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from neptune) (1.8.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from neptune) (21.3)\n",
      "Requirement already satisfied: future>=0.17.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from neptune) (1.0.0)\n",
      "Requirement already satisfied: boto3>=1.28.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from neptune) (1.39.2)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from neptune) (2.4.0)\n",
      "Requirement already satisfied: requests-oauthlib>=1.0.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from neptune) (2.0.0)\n",
      "Requirement already satisfied: bravado<12.0.0,>=11.0.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from neptune) (11.1.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from neptune) (2.2.2)\n",
      "Requirement already satisfied: PyJWT in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from neptune) (2.10.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from neptune) (5.9.7)\n",
      "Requirement already satisfied: botocore<1.40.0,>=1.39.2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from boto3>=1.28.0->neptune) (1.39.2)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from boto3>=1.28.0->neptune) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from boto3>=1.28.0->neptune) (0.13.0)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from bravado<12.0.0,>=11.0.0->neptune) (2.8.2)\n",
      "Requirement already satisfied: simplejson in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from bravado<12.0.0,>=11.0.0->neptune) (3.20.1)\n",
      "Requirement already satisfied: bravado-core>=5.16.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from bravado<12.0.0,>=11.0.0->neptune) (6.1.1)\n",
      "Requirement already satisfied: msgpack in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from bravado<12.0.0,>=11.0.0->neptune) (1.1.1)\n",
      "Requirement already satisfied: monotonic in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from bravado<12.0.0,>=11.0.0->neptune) (1.6)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from bravado<12.0.0,>=11.0.0->neptune) (6.0.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from click>=7.0->neptune) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from GitPython>=2.0.8->neptune) (4.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests>=2.20.0->neptune) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests>=2.20.0->neptune) (2025.4.26)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests>=2.20.0->neptune) (3.4.1)\n",
      "Requirement already satisfied: jsonschema in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from swagger-spec-validator>=2.7.4->neptune) (4.23.0)\n",
      "Requirement already satisfied: importlib-resources>=1.3 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from swagger-spec-validator>=2.7.4->neptune) (6.5.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from packaging->neptune) (3.0.9)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas->neptune) (2024.1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas->neptune) (1.26.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas->neptune) (2024.1)\n",
      "Requirement already satisfied: jsonref in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune) (1.1.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gitdb<5,>=4.0.1->GitPython>=2.0.8->neptune) (5.0.2)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (0.24.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (2025.4.1)\n",
      "Requirement already satisfied: rfc3339-validator in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>0.1.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (0.1.1)\n",
      "Requirement already satisfied: jsonpointer>1.13 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (3.0.0)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (24.11.1)\n",
      "Requirement already satisfied: uri-template in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (1.3.0)\n",
      "Requirement already satisfied: fqdn in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (1.5.1)\n",
      "Requirement already satisfied: isoduration in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (20.11.0)\n",
      "Requirement already satisfied: arrow>=0.15.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from isoduration->jsonschema->swagger-spec-validator>=2.7.4->neptune) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from arrow>=0.15.0->isoduration->jsonschema->swagger-spec-validator>=2.7.4->neptune) (2.9.0.20250516)\n",
      "Installing collected packages: neptune\n",
      "Successfully installed neptune-1.14.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\Admin\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall neptune neptune-client -y\n",
    "%pip install neptune\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67f4b5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import neptune\n",
    "import praw\n",
    "from datetime import datetime\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1296a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_progress(filename=\"scraping_progress.json\"):\n",
    "    \"\"\"Load previously scraped data if it exists\"\"\"\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            return data.get('posts', []), data.get('last_post_id', None)\n",
    "    except FileNotFoundError:\n",
    "        return [], None\n",
    "\n",
    "def save_progress(posts_data, last_post_id, filename=\"scraping_progress.json\"):\n",
    "    \"\"\"Save current progress to file\"\"\"\n",
    "    progress_data = {\n",
    "        'posts': posts_data,\n",
    "        'last_post_id': last_post_id,\n",
    "        'saved_at': datetime.now().isoformat(),\n",
    "        'total_posts': len(posts_data)\n",
    "    }\n",
    "    # Save progress as JSON (for resume functionality)\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(progress_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # Also save current data as CSV\n",
    "    if posts_data:\n",
    "        df = pd.DataFrame(posts_data)\n",
    "        df.to_csv(\"wallstreetbets_posts.csv\", index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cf445da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_final_csv(posts_data):\n",
    "    \"\"\"Save final dataset as CSV with proper formatting\"\"\"\n",
    "    if not posts_data:\n",
    "        return\n",
    "        \n",
    "    df = pd.DataFrame(posts_data)\n",
    "    \n",
    "    # Convert timestamp to readable format\n",
    "    df['created_datetime'] = pd.to_datetime(df['created_utc'], unit='s')\n",
    "    \n",
    "    # Reorder columns for better readability\n",
    "    columns_order = ['post_id', 'title', 'text', 'post_type', 'author_name', 'score', 'num_comments', \n",
    "                    'created_utc', 'created_datetime', 'url']\n",
    "    df = df[columns_order]\n",
    "    \n",
    "    # Save with timestamp in filename\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    csv_filename = f\"wallstreetbets_posts_{timestamp}.csv\"\n",
    "    df.to_csv(csv_filename, index=False, encoding='utf-8')\n",
    "    \n",
    "    return csv_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9a4b05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reddit_connect():\n",
    "    \"\"\"Initialize Reddit connection\"\"\"\n",
    "    load_dotenv()\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=os.getenv('REDDIT_CLIENT_ID'),\n",
    "        client_secret=os.getenv('REDDIT_CLIENT_SECRET'),\n",
    "        user_agent=os.getenv('REDDIT_USER_AGENT'),\n",
    "        username=os.getenv('REDDIT_USERNAME'),\n",
    "        password=os.getenv('REDDIT_PASSWORD')\n",
    "    )\n",
    "    return reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2d3f780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(reddit, target_posts=50):\n",
    "    \"\"\"Build the dataset by scraping WallStreetBets posts\"\"\"\n",
    "    \n",
    "    # Load previous progress\n",
    "    posts_data, last_post_id = load_progress()\n",
    "    start_count = len(posts_data)\n",
    "    \n",
    "    if start_count > 0:\n",
    "        print(f\"Resuming from {start_count} previously scraped posts...\")\n",
    "    else:\n",
    "        print(\"Starting fresh scrape...\")\n",
    "    \n",
    "    # Get subreddit\n",
    "    subreddit = reddit.subreddit(\"wallstreetbets\")\n",
    "    \n",
    "    # Track scraping metrics\n",
    "    start_time = time.time()\n",
    "    errors_count = 0\n",
    "    \n",
    "    try:\n",
    "        # Get posts (PRAW handles pagination automatically)\n",
    "        posts_generator = subreddit.new(limit=target_posts)\n",
    "        \n",
    "        # Convert to list to get total count for progress bar\n",
    "        print(\"Fetching post list from Reddit...\")\n",
    "        all_posts = list(posts_generator)\n",
    "        \n",
    "        # Skip posts we already have if resuming\n",
    "        if last_post_id:\n",
    "            # Find where to resume\n",
    "            resume_index = 0\n",
    "            for i, post in enumerate(all_posts):\n",
    "                if post.id == last_post_id:\n",
    "                    resume_index = i + 1\n",
    "                    break\n",
    "            all_posts = all_posts[resume_index:]\n",
    "            print(f\"Resuming from post index {resume_index}\")\n",
    "        \n",
    "        # Process remaining posts\n",
    "        posts_to_process = min(len(all_posts), target_posts - start_count)\n",
    "        \n",
    "        for i, submission in enumerate(tqdm(all_posts[:posts_to_process], \n",
    "                                        desc=\"Scraping posts\", \n",
    "                                        initial=start_count, \n",
    "                                        total=target_posts)):\n",
    "            try:\n",
    "                # Handle author safely\n",
    "                author_name = \"[deleted]\"\n",
    "                if submission.author is not None:\n",
    "                    try:\n",
    "                        author_name = submission.author.name\n",
    "                    except Exception:\n",
    "                        author_name = \"[unavailable]\"\n",
    "                \n",
    "                # Extract post text content with better categorization\n",
    "                post_text = \"\"\n",
    "                post_type = \"text\"  # Default type\n",
    "                \n",
    "                if submission.is_self:  # Text post\n",
    "                    if submission.selftext:\n",
    "                        post_text = submission.selftext\n",
    "                        post_type = \"text\"\n",
    "                    else:\n",
    "                        post_text = \"[Empty text post]\"\n",
    "                        post_type = \"text_empty\"\n",
    "                else:  # Link post\n",
    "                    post_text = \"[Link Post]\"\n",
    "                    post_type = \"link\"\n",
    "                    \n",
    "                    # You could also categorize by URL type\n",
    "                    if any(img_ext in submission.url.lower() for img_ext in ['.jpg', '.jpeg', '.png', '.gif']):\n",
    "                        post_type = \"image\"\n",
    "                    elif 'youtube.com' in submission.url.lower() or 'youtu.be' in submission.url.lower():\n",
    "                        post_type = \"video\"\n",
    "                \n",
    "                # Extract post data (keeping titles and emojis intact)\n",
    "                post_info = {\n",
    "                    \"post_id\": submission.id,\n",
    "                    \"title\": submission.title,  # Preserves emojis and formatting\n",
    "                    \"text\": post_text,  # The actual post content\n",
    "                    \"post_type\": post_type,  # Type of post for analysis\n",
    "                    \"score\": submission.score,\n",
    "                    \"created_utc\": submission.created_utc,\n",
    "                    \"num_comments\": submission.num_comments,\n",
    "                    \"url\": submission.url,\n",
    "                    \"author_name\": author_name\n",
    "                }\n",
    "                \n",
    "                posts_data.append(post_info)\n",
    "                \n",
    "                # Save progress every 50 posts\n",
    "                if (len(posts_data) - start_count) % 50 == 0:\n",
    "                    save_progress(posts_data, submission.id)\n",
    "                \n",
    "                # Small delay every 100 posts to be nice to Reddit\n",
    "                if (len(posts_data) - start_count) % 100 == 0:\n",
    "                    time.sleep(1)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                errors_count += 1\n",
    "                print(f\"Error processing post {submission.id}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Final save\n",
    "        save_progress(posts_data, posts_data[-1][\"post_id\"] if posts_data else None)\n",
    "        \n",
    "        # Calculate final metrics\n",
    "        end_time = time.time()\n",
    "        scraping_duration = end_time - start_time\n",
    "        \n",
    "        # Save final CSV dataset\n",
    "        csv_filename = save_final_csv(posts_data)\n",
    "        \n",
    "        # Return metrics for tracking\n",
    "        metrics = {\n",
    "            'total_posts_collected': len(posts_data),\n",
    "            'new_posts_this_session': len(posts_data) - start_count,\n",
    "            'scraping_duration_minutes': scraping_duration / 60,\n",
    "            'errors_encountered': errors_count,\n",
    "            'posts_per_minute': (len(posts_data) - start_count) / (scraping_duration / 60) if scraping_duration > 0 else 0,\n",
    "            'csv_filename': csv_filename,\n",
    "            'total_posts_available': len(all_posts),\n",
    "            'resumed_from': start_count if start_count > 0 else None\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nDataset building completed!\")\n",
    "        print(f\"Total posts collected: {metrics['total_posts_collected']}\")\n",
    "        print(f\"New posts this session: {metrics['new_posts_this_session']}\")\n",
    "        print(f\"Duration: {metrics['scraping_duration_minutes']:.2f} minutes\")\n",
    "        print(f\"Errors: {metrics['errors_encountered']}\")\n",
    "        \n",
    "        return posts_data, metrics\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nScraping interrupted by user. Progress saved.\")\n",
    "        save_progress(posts_data, posts_data[-1][\"post_id\"] if posts_data else None)\n",
    "        return posts_data, {'interrupted': True, 'posts_at_interruption': len(posts_data)}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {str(e)}\")\n",
    "        save_progress(posts_data, posts_data[-1][\"post_id\"] if posts_data else None)\n",
    "        return posts_data, {'fatal_error': str(e), 'posts_at_error': len(posts_data)}\n",
    "\n",
    "def track_scraping_metrics(posts_data, metrics, target_posts):\n",
    "    \"\"\"Track scraping metrics and results in Neptune\"\"\"\n",
    "    # Initialize Neptune\n",
    "    run = neptune.init_run(project=\"krishnadasm/wallstreetbets-scraper\")\n",
    "    \n",
    "    # Log configuration\n",
    "    run[\"config/target_posts\"] = target_posts\n",
    "    run[\"config/subreddit\"] = \"wallstreetbets\"\n",
    "    run[\"config/sort_method\"] = \"new\"\n",
    "    run[\"config/resume_enabled\"] = True\n",
    "    \n",
    "    # Log scraping metrics\n",
    "    if 'interrupted' in metrics:\n",
    "        run[\"scraping/interrupted\"] = True\n",
    "        run[\"scraping/posts_at_interruption\"] = metrics['posts_at_interruption']\n",
    "    elif 'fatal_error' in metrics:\n",
    "        run[\"scraping/fatal_error\"] = metrics['fatal_error']\n",
    "        run[\"scraping/posts_at_error\"] = metrics['posts_at_error']\n",
    "    else:\n",
    "        # Log successful completion metrics\n",
    "        run[\"results/total_posts_collected\"] = metrics['total_posts_collected']\n",
    "        run[\"results/new_posts_this_session\"] = metrics['new_posts_this_session']\n",
    "        run[\"results/scraping_duration_minutes\"] = metrics['scraping_duration_minutes']\n",
    "        run[\"results/errors_encountered\"] = metrics['errors_encountered']\n",
    "        run[\"results/posts_per_minute\"] = metrics['posts_per_minute']\n",
    "        run[\"scraping/total_posts_available\"] = metrics['total_posts_available']\n",
    "        \n",
    "        if metrics['resumed_from']:\n",
    "            run[\"scraping/resumed_from\"] = metrics['resumed_from']\n",
    "        \n",
    "        # Upload final dataset to Neptune\n",
    "        if metrics.get('csv_filename'):\n",
    "            run[\"data/posts_dataset\"].upload(metrics['csv_filename'])\n",
    "        run[\"data/progress_file\"].upload(\"scraping_progress.json\")\n",
    "    \n",
    "    # Log final progress\n",
    "    run[\"scraping/final_progress\"] = len(posts_data)\n",
    "    \n",
    "    run.stop()\n",
    "    print(\"Metrics logged to Neptune successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "788993d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Reddit API successfully.\n",
      "Resuming from 50 previously scraped posts...\n",
      "Fetching post list from Reddit...\n",
      "Resuming from post index 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping posts: 100%|██████████| 50/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error: \"['text', 'post_type'] not in index\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[neptune] [warning] NeptuneWarning: By default, these monitoring options are disabled in interactive sessions: 'capture_stdout', 'capture_stderr', 'capture_traceback', 'capture_hardware_metrics'. You can set them to 'True' when initializing the run and the monitoring will continue until you call run.stop() or the kernel stops. NOTE: To track the source files, pass their paths to the 'source_code' argument. For help, see: https://docs-legacy.neptune.ai/logging/source_code/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/krishnadasm/wallstreetbets-scraper/e/WAL-11\n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "[neptune] [info   ] All 14 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/krishnadasm/wallstreetbets-scraper/e/WAL-11/metadata\n",
      "Metrics logged to Neptune successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def scrape_wallstreetbets():\n",
    "    \"\"\"Main function that orchestrates the scraping process\"\"\"\n",
    "    # Build the dataset\n",
    "    reddit = reddit_connect()\n",
    "    print(\"Connected to Reddit API successfully.\")\n",
    "    posts_data, metrics = build_dataset(reddit, target_posts=50)\n",
    "    \n",
    "    # Track metrics in Neptune\n",
    "    track_scraping_metrics(posts_data, metrics, target_posts=50)\n",
    "    \n",
    "    return posts_data, metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_wallstreetbets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecae9cd",
   "metadata": {},
   "source": [
    "Environments setup for Reddit and neptune.ai secrets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf2ca58",
   "metadata": {},
   "source": [
    "### Neptune.ai Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bccee61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token loaded: eyJhcGlfYW...\n",
      "https://app.neptune.ai/krishnadasm/wallstreetbets-scraper/e/WAL-4\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 1 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 1 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/krishnadasm/wallstreetbets-scraper/e/WAL-4/metadata\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# Test that it loaded\n",
    "print(\"Token loaded:\", os.getenv('NEPTUNE_API_TOKEN')[:10] + \"...\")  # Only shows first 10 chars\n",
    "\n",
    "# neptune_api_token = user_secrets.get_secret(\"neptune_api\")\n",
    "run = None\n",
    "try:\n",
    "    run = neptune.init_run(\n",
    "    project=\"krishnadasm/wallstreetbets-scraper\"\n",
    "    )\n",
    "    run[\"test\"] = \"Connected with .env file!\"\n",
    "    run.stop()\n",
    "except Exception as ex:\n",
    "    print(f\"Exception: {ex}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97a28f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install python-dotenv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
