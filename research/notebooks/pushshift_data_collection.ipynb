{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7f76269",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50e4d54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import neptune\n",
    "import praw\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "Root = Path('.').absolute().parent\n",
    "SCRIPTS = Root / r'scripts'\n",
    "# SCRIPTS = Root / r'C:\\Users\\Admin\\Projects\\ML Projects\\ManipDetect\\research\\scripts'\n",
    "DATA = Root/ r'C:\\Users\\krishnadas\\Projects\\ML Projects\\ManipDetect\\data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be06c047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reddit_connect():\n",
    "    \"\"\"Initialize Reddit connection\"\"\"\n",
    "    load_dotenv()\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=os.getenv('REDDIT_CLIENT_ID'),\n",
    "        client_secret=os.getenv('REDDIT_CLIENT_SECRET'),\n",
    "        user_agent=os.getenv('REDDIT_USER_AGENT'),\n",
    "        username=os.getenv('REDDIT_USERNAME'),\n",
    "        password=os.getenv('REDDIT_PASSWORD')\n",
    "    )\n",
    "    return reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd7c6d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_progress(filename=\"scraping_progress.json\"):\n",
    "    \"\"\"Load previously scraped data if it exists\"\"\"\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            posts = data.get('posts', [])\n",
    "            \n",
    "            # Normalize old data structure to ensure consistency\n",
    "            consistent_posts = []\n",
    "            for post in posts:\n",
    "                # Ensure all required fields exist with default values\n",
    "                consistent_post_dict = {\n",
    "                    'post_id': post.get('post_id', ''),\n",
    "                    'title': post.get('title', ''),\n",
    "                    'text': post.get('text', post.get('selftext', '')),  # Handle old 'selftext' field\n",
    "                    'post_type': post.get('post_type', 'unknown'),\n",
    "                    'author_name': post.get('author_name', post.get('author', '[unknown]')),  # Handle old 'author' field\n",
    "                    'author_id':post.get('author_id', post.get('author_id', '')),\n",
    "                    'score': post.get('score', 0),\n",
    "                    'num_comments': post.get('num_comments', 0),\n",
    "                    'created_utc': post.get('created_utc', 0),\n",
    "                    'url': post.get('url', '')\n",
    "                }\n",
    "                consistent_posts.append(consistent_post_dict)\n",
    "            scraped_ids = [post.get('post_id') for post in consistent_posts]\n",
    "            return consistent_posts, scraped_ids\n",
    "    except FileNotFoundError:\n",
    "        return [], set()\n",
    "\n",
    "def save_progress(posts_data, scraped_ids, filename=\"scraping_progress.json\"):\n",
    "    \"\"\"Save current progress to file\"\"\"\n",
    "    progress_data = {\n",
    "        'posts': posts_data,\n",
    "        'scraped_ids': scraped_ids,\n",
    "        'saved_at': datetime.now().isoformat(),\n",
    "        'total_posts': len(posts_data)\n",
    "    }\n",
    "    # Save progress as JSON (for resume functionality)\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(progress_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # Also save current data as CSV\n",
    "    if posts_data:\n",
    "        df = pd.DataFrame(posts_data)\n",
    "        df.to_csv(\"wallstreetbetsnew_posts.csv\", index=False, encoding='utf-8')\n",
    "\n",
    "def save_final_csv(posts_data, filepath):\n",
    "    \"\"\"Save final dataset as CSV with proper formatting\"\"\"\n",
    "    if not posts_data:\n",
    "        return\n",
    "    df = pd.DataFrame(posts_data)\n",
    "    \n",
    "    # Convert timestamp to readable format\n",
    "    df['created_datetime'] = pd.to_datetime(df['created_utc'], unit='s')\n",
    "    \n",
    "    # Reorder columns for better readability, but only use columns that exist\n",
    "    preferred_columns_order = ['post_id', 'title', 'text', 'post_type', 'author_name', 'author_id', 'score', 'num_comments', \n",
    "                                'created_utc', 'url']\n",
    "    \n",
    "    # Filter to only include columns that actually exist in the DataFrame\n",
    "    available_columns = [col for col in preferred_columns_order if col in df.columns]\n",
    "    \n",
    "    # Add any remaining columns that weren't in our preferred order\n",
    "    remaining_columns = [col for col in df.columns if col not in available_columns]\n",
    "    final_columns_order = available_columns + remaining_columns\n",
    "    \n",
    "    df = df[final_columns_order]\n",
    "    \n",
    "    # Save with timestamp in filename\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    # csv_filename = f\"wallstreetbetsnew_posts_{timestamp}.csv\"\n",
    "    csv_filename = filepath / f\"wallstreetbetsnew_posts_{timestamp}.csv\"\n",
    "    df.to_csv(csv_filename, index=False, encoding='utf-8')\n",
    "    \n",
    "    return csv_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95726a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_to_unix(date_string):\n",
    "    \"\"\"Convert date string to Unix timestamp\"\"\"\n",
    "    return int(datetime.strptime(date_string, '%Y-%m-%d').timestamp())\n",
    "\n",
    "def unix_to_date(unix_timestamp):\n",
    "    \"\"\"Convert Unix timestamp to readable date\"\"\"\n",
    "    return datetime.fromtimestamp(unix_timestamp, timezone.utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "def _extract_post_data(submission):\n",
    "        \"\"\"Extract comprehensive post data\"\"\"\n",
    "        \n",
    "        # Handle author\n",
    "        author_name = \"[deleted]\"\n",
    "        author_id = \"[deleted]\"\n",
    "        if submission.author is not None:\n",
    "            try:\n",
    "                author_name = submission.author.name\n",
    "                author_id = submission.author.id\n",
    "            except:\n",
    "                author_name = \"[unavailable]\"\n",
    "                author_id = \"[unavailable]\"\n",
    "        \n",
    "        # Handle post text\n",
    "        post_text = \"\"\n",
    "        post_type = \"text\"\n",
    "        \n",
    "        if submission.is_self:\n",
    "            if submission.selftext:\n",
    "                post_text = submission.selftext\n",
    "                post_type = \"text\"\n",
    "            else:\n",
    "                post_text = \"[Empty text post]\"\n",
    "                post_type = \"text_empty\"\n",
    "        else:\n",
    "            post_text = \"[Link Post]\"\n",
    "            post_type = \"link\"\n",
    "            \n",
    "            # Categorize link types\n",
    "            if any(ext in submission.url.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif']):\n",
    "                post_type = \"image\"\n",
    "            elif any(site in submission.url.lower() for site in ['youtube.com', 'youtu.be']):\n",
    "                post_type = \"video\"\n",
    "        \n",
    "        return {\n",
    "            \"post_id\": submission.id,\n",
    "            \"title\": submission.title,\n",
    "            \"text\": post_text,\n",
    "            \"post_type\": post_type,\n",
    "            \"author_name\": author_name,\n",
    "            \"author_id\": author_id,\n",
    "            \"score\": submission.score,\n",
    "            # \"upvote_ratio\": getattr(submission, 'upvote_ratio', None),\n",
    "            \"num_comments\": submission.num_comments,\n",
    "            \"created_utc\": submission.created_utc,\n",
    "            # \"created_datetime\": unix_to_date(submission.created_utc),\n",
    "            \"url\": submission.url\n",
    "            # \"permalink\": f\"https://reddit.com{submission.permalink}\",\n",
    "            # \"subreddit\": submission.subreddit.display_name,\n",
    "            # \"gilded\": submission.gilded,\n",
    "            # \"locked\": submission.locked,\n",
    "            # \"over_18\": submission.over_18,\n",
    "            # \"spoiler\": submission.spoiler,\n",
    "            # \"stickied\": submission.stickied\n",
    "        }\n",
    "\n",
    "def _is_same_session(self, current_config, saved_config):\n",
    "    \"\"\"Check if current scraping session matches saved session\"\"\"\n",
    "    key_fields = ['method', 'subreddit', 'start_date', 'end_date', \n",
    "                    'max_posts', 'sort_method', 'keywords', 'days_back']\n",
    "    \n",
    "    for field in key_fields:\n",
    "        if current_config.get(field) != saved_config.get(field):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5508a942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_by_date_range(subreddit_name, start_date, end_date, \n",
    "                        max_posts=None, sort_method='new', resume=True, batch_size = 500):\n",
    "        \"\"\"\n",
    "        Scrape posts from a specific date range with batch processing and resume capability\n",
    "        \"\"\"\n",
    "        print(f\"Scraping r/{subreddit_name} from {start_date} to {end_date}\")\n",
    "        \n",
    "        # Configuration for this scraping session\n",
    "        config = {\n",
    "            'method': 'date_range',\n",
    "            'subreddit': subreddit_name,\n",
    "            'start_date': start_date,\n",
    "            'end_date': end_date,\n",
    "            'max_posts': max_posts,\n",
    "            'sort_method': sort_method,\n",
    "            'batch_size': batch_size\n",
    "        }\n",
    "        filepath = SCRIPTS/'temp_data'  # Define the path to save progress\n",
    "        # Load previous progress if resuming\n",
    "        if resume:\n",
    "            posts_data, scraped_ids, saved_config = load_progress()\n",
    "            \n",
    "            # Check if we're resuming the same scraping session\n",
    "            if saved_config and _is_same_session(config, saved_config):\n",
    "                print(f\"Resuming previous session with {len(posts_data)} posts\")\n",
    "                print(f\"Will skip {len(scraped_ids)} already scraped post IDs\")\n",
    "                start_count = len(posts_data)\n",
    "            else:\n",
    "                print(\"Starting new scraping session\")\n",
    "                posts_data, scraped_ids = [], set()\n",
    "                start_count = 0\n",
    "        else:\n",
    "            print(\"Starting fresh scraping session (no resume)\")\n",
    "            posts_data, scraped_ids = [], set()\n",
    "            start_count = 0\n",
    "        \n",
    "        # Convert dates to Unix timestamps\n",
    "        start_timestamp = date_to_unix(start_date)\n",
    "        end_timestamp = date_to_unix(end_date)\n",
    "        \n",
    "        reddit = reddit_connect()\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "        \n",
    "        # Get posts based on sort method\n",
    "        if sort_method == 'new':\n",
    "            posts_generator = subreddit.new(limit=None)\n",
    "        elif sort_method == 'hot':\n",
    "            posts_generator = subreddit.hot(limit=None)\n",
    "        elif sort_method == 'top':\n",
    "            posts_generator = subreddit.top(time_filter='all', limit=None)\n",
    "        else:\n",
    "            raise ValueError(\"sort_method must be 'new', 'hot', or 'top'\")\n",
    "        \n",
    "        print(\"Fetching posts from Reddit...\")\n",
    "        collected_posts = 0\n",
    "        batch_count = 0\n",
    "        errors_count = 0\n",
    "        \n",
    "        try:\n",
    "            for submission in tqdm(posts_generator, desc=\"Processing posts\"):\n",
    "                # Skip if already scraped\n",
    "                if submission.id in scraped_ids:\n",
    "                    continue\n",
    "                \n",
    "                # Check if post is within date range\n",
    "                if submission.created_utc < start_timestamp:\n",
    "                    if sort_method == 'new':\n",
    "                        print(f\"Reached posts older than {start_date}, stopping...\")\n",
    "                        break\n",
    "                    else:\n",
    "                        continue\n",
    "                \n",
    "                if submission.created_utc > end_timestamp:\n",
    "                    continue\n",
    "                \n",
    "                # Extract post data\n",
    "                try:\n",
    "                    post_data = _extract_post_data(submission)\n",
    "                    posts_data.append(post_data)\n",
    "                    scraped_ids.add(submission.id)\n",
    "                    collected_posts += 1\n",
    "                    \n",
    "                    # Batch-wise progress saving\n",
    "                    if collected_posts % batch_size == 0:\n",
    "                        batch_count += 1\n",
    "                        print(f\"\\nCompleted batch {batch_count} ({collected_posts} posts)\")\n",
    "                        save_progress(posts_data, scraped_ids, config)\n",
    "                        \n",
    "                        # Rate limiting between batches\n",
    "                        print(\"Taking 30-second break between batches...\")\n",
    "                        time.sleep(30)\n",
    "                    \n",
    "                    # Stop if we've reached max_posts\n",
    "                    if max_posts and collected_posts >= max_posts:\n",
    "                        print(f\"Reached target of {max_posts} posts\")\n",
    "                        break\n",
    "                    \n",
    "                    # Rate limiting within batch\n",
    "                    if collected_posts % 50 == 0:\n",
    "                        time.sleep(1)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    errors_count += 1\n",
    "                    print(f\"Error processing post {submission.id}: {e}\")\n",
    "                    if errors_count > 50:  # Stop if too many errors\n",
    "                        print(\"Too many errors, stopping...\")\n",
    "                        break\n",
    "                    continue\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nScraping interrupted by user\")\n",
    "            save_progress(posts_data, scraped_ids, config)\n",
    "            return posts_data, config\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {e}\")\n",
    "            save_progress(posts_data, scraped_ids, config)\n",
    "            return posts_data, config\n",
    "        \n",
    "        # Final save\n",
    "        save_final_csv(posts_data, filepath)\n",
    "        \n",
    "        new_posts = collected_posts\n",
    "        total_posts = len(posts_data)\n",
    "        \n",
    "        print(f\"\\nScraping completed!\")\n",
    "        print(f\"New posts this session: {new_posts}\")\n",
    "        print(f\"Total posts collected: {total_posts}\")\n",
    "        print(f\"Errors encountered: {errors_count}\")\n",
    "        print(f\"Date range: {start_date} to {end_date}\")\n",
    "        \n",
    "        return posts_data, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80edd7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_historical_batch(subreddit_name, days_back=30, \n",
    "                            posts_per_day=100, sort_method='new'):\n",
    "        \"\"\"\n",
    "        Scrape historical data in daily batches (Pushshift-style)\n",
    "        \n",
    "        Args:\n",
    "            subreddit_name: Name of subreddit\n",
    "            days_back: How many days back to scrape\n",
    "            posts_per_day: Target posts per day\n",
    "            sort_method: Sorting method\n",
    "        \"\"\"\n",
    "        print(f\"Scraping {days_back} days of historical data\")\n",
    "        \n",
    "        all_posts = []\n",
    "        end_date = datetime.now()\n",
    "        \n",
    "        for day in range(days_back):\n",
    "            current_date = end_date - timedelta(days=day)\n",
    "            start_date = current_date - timedelta(days=1)\n",
    "            \n",
    "            start_str = start_date.strftime('%Y-%m-%d')\n",
    "            end_str = current_date.strftime('%Y-%m-%d')\n",
    "            \n",
    "            print(f\"\\nScraping day {day + 1}/{days_back}: {start_str}\")\n",
    "            \n",
    "            day_posts = scrape_by_date_range(\n",
    "                subreddit_name, start_str, end_str, \n",
    "                max_posts=posts_per_day, sort_method=sort_method\n",
    "            )\n",
    "            \n",
    "            all_posts.extend(day_posts)\n",
    "            \n",
    "            # Longer break between days\n",
    "            if day < days_back - 1:\n",
    "                print(\"Waiting 30 seconds before next day...\")\n",
    "                time.sleep(30)\n",
    "        \n",
    "        return all_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22176e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_with_keywords(subreddit_name, keywords, max_posts=1000, \n",
    "                        days_back=7):\n",
    "    \"\"\"\n",
    "    Scrape posts containing specific keywords (Pushshift-style search)\n",
    "    \n",
    "    Args:\n",
    "        subreddit_name: Subreddit name\n",
    "        keywords: List of keywords to search for\n",
    "        max_posts: Maximum posts to return\n",
    "        days_back: Days to look back\n",
    "    \"\"\"\n",
    "    print(f\"Searching for posts with keywords: {keywords}\")\n",
    "    \n",
    "    # Get recent posts\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=days_back)\n",
    "    \n",
    "    start_str = start_date.strftime('%Y-%m-%d')\n",
    "    end_str = end_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    all_posts = scrape_by_date_range(\n",
    "        subreddit_name, start_str, end_str, \n",
    "        max_posts=max_posts * 5  # Get more to filter\n",
    "    )\n",
    "    \n",
    "    # Filter by keywords\n",
    "    filtered_posts = []\n",
    "    for post in all_posts:\n",
    "        title_lower = post['title'].lower()\n",
    "        text_lower = post['text'].lower()\n",
    "        \n",
    "        # Check if any keyword is in title or text\n",
    "        if any(keyword.lower() in title_lower or keyword.lower() in text_lower \n",
    "                for keyword in keywords):\n",
    "            filtered_posts.append(post)\n",
    "            \n",
    "            if len(filtered_posts) >= max_posts:\n",
    "                break\n",
    "    \n",
    "    print(f\"Found {len(filtered_posts)} posts with keywords\")\n",
    "    return filtered_posts\n",
    "\n",
    "def _extract_post_data(submission):\n",
    "    \"\"\"Extract comprehensive post data\"\"\"\n",
    "    \n",
    "    # Handle author\n",
    "    author_name = \"[deleted]\"\n",
    "    author_id = \"[deleted]\"\n",
    "    if submission.author is not None:\n",
    "        try:\n",
    "            author_name = submission.author.name\n",
    "            author_id = submission.author.id\n",
    "        except:\n",
    "            author_name = \"[unavailable]\"\n",
    "            author_id = \"[unavailable]\"\n",
    "    \n",
    "    # Handle post text\n",
    "    post_text = \"\"\n",
    "    post_type = \"text\"\n",
    "    \n",
    "    if submission.is_self:\n",
    "        if submission.selftext:\n",
    "            post_text = submission.selftext\n",
    "            post_type = \"text\"\n",
    "        else:\n",
    "            post_text = \"[Empty text post]\"\n",
    "            post_type = \"text_empty\"\n",
    "    else:\n",
    "        post_text = \"[Link Post]\"\n",
    "        post_type = \"link\"\n",
    "        \n",
    "        # Categorize link types\n",
    "        if any(ext in submission.url.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif']):\n",
    "            post_type = \"image\"\n",
    "        elif any(site in submission.url.lower() for site in ['youtube.com', 'youtu.be']):\n",
    "            post_type = \"video\"\n",
    "    \n",
    "    return {\n",
    "        \"post_id\": submission.id,\n",
    "        \"title\": submission.title,\n",
    "        \"text\": post_text,\n",
    "        \"post_type\": post_type,\n",
    "        \"author_name\": author_name,\n",
    "        \"author_id\": author_id,\n",
    "        \"score\": submission.score,\n",
    "        \"upvote_ratio\": getattr(submission, 'upvote_ratio', None),\n",
    "        \"num_comments\": submission.num_comments,\n",
    "        \"created_utc\": submission.created_utc,\n",
    "        \"created_datetime\": unix_to_date(submission.created_utc),\n",
    "        \"url\": submission.url,\n",
    "        \"permalink\": f\"https://reddit.com{submission.permalink}\",\n",
    "        \"subreddit\": submission.subreddit.display_name,\n",
    "        \"gilded\": submission.gilded,\n",
    "        \"locked\": submission.locked,\n",
    "        \"over_18\": submission.over_18,\n",
    "        \"spoiler\": submission.spoiler,\n",
    "        \"stickied\": submission.stickied\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4dce681b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(posts_data, filename=None):\n",
    "    \"\"\"Save posts data to CSV\"\"\"\n",
    "    if not filename:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"reddit_posts_{timestamp}.csv\"\n",
    "    \n",
    "    df = pd.DataFrame(posts_data)\n",
    "    df.to_csv(filename, index=False, encoding='utf-8')\n",
    "    print(f\"Saved {len(posts_data)} posts to {filename}\")\n",
    "    return filename\n",
    "\n",
    "def track_with_neptune(posts_data, config):\n",
    "    \"\"\"Track scraping results with Neptune\"\"\"\n",
    "    run = neptune.init_run(project=\"krishnadasm/wallstreetbets-scraper\")\n",
    "    \n",
    "    # Log configuration\n",
    "    for key, value in config.items():\n",
    "        run[f\"config/{key}\"] = value\n",
    "    \n",
    "    # Log results\n",
    "    run[\"results/total_posts\"] = len(posts_data)\n",
    "    run[\"results/date_range\"] = f\"{config.get('start_date', 'N/A')} to {config.get('end_date', 'N/A')}\"\n",
    "    \n",
    "    # Analyze post types\n",
    "    if posts_data:\n",
    "        df = pd.DataFrame(posts_data)\n",
    "        post_type_counts = df['post_type'].value_counts().to_dict()\n",
    "        for post_type, count in post_type_counts.items():\n",
    "            run[f\"analysis/post_types/{post_type}\"] = count\n",
    "    \n",
    "    # Upload CSV\n",
    "    csv_filename = save_to_csv(posts_data)\n",
    "    run[\"data/posts_csv\"].upload(csv_filename)\n",
    "    \n",
    "    run.stop()\n",
    "    return csv_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "527b4c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_historical_scraping():\n",
    "    \"\"\"Example: Scrape historical data day by day\"\"\"\n",
    "    # scraper = PushshiftStyleScraper()\n",
    "    \n",
    "    posts = scrape_historical_batch(\n",
    "        subreddit_name='wallstreetbets',\n",
    "        days_back=2,\n",
    "        posts_per_day=100,\n",
    "        sort_method='new'\n",
    "    )\n",
    "    \n",
    "    config = {\n",
    "        'method': 'historical_batch',\n",
    "        'subreddit': 'wallstreetbets',\n",
    "        'days_back': 2,\n",
    "        'posts_per_day': 100\n",
    "    }\n",
    "    \n",
    "    track_with_neptune(posts, config)\n",
    "\n",
    "def example_keyword_search():\n",
    "    \"\"\"Example: Search for posts with specific keywords\"\"\"\n",
    "    # scraper = PushshiftStyleScraper()\n",
    "    \n",
    "    posts = scrape_with_keywords(\n",
    "        subreddit_name='wallstreetbets',\n",
    "        keywords=['GME', 'GameStop', 'TSLA', 'Tesla'],\n",
    "        max_posts=500,\n",
    "        days_back=7\n",
    "    )\n",
    "    \n",
    "    config = {\n",
    "        'method': 'keyword_search',\n",
    "        'subreddit': 'wallstreetbetsnew',\n",
    "        'keywords': ['GME', 'GameStop', 'TSLA', 'Tesla'],\n",
    "        'max_posts': 500,\n",
    "        'days_back': 7\n",
    "    }\n",
    "    \n",
    "    track_with_neptune(posts, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1f28dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushshift-Style Reddit Scraper\n",
      "==================================================\n",
      "Scraping 2 days of historical data\n",
      "\n",
      "Scraping day 1/2: 2025-07-08\n",
      "üîç Scraping r/wallstreetbets from 2025-07-08 to 2025-07-09\n",
      "Fetching posts from Reddit...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering by date: 34it [00:07,  4.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 20 posts from 2025-07-08 to 2025-07-09\n",
      "Waiting 30 seconds before next day...\n",
      "\n",
      "Scraping day 2/2: 2025-07-07\n",
      "üîç Scraping r/wallstreetbets from 2025-07-07 to 2025-07-08\n",
      "Fetching posts from Reddit...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering by date: 52it [00:06,  7.72it/s]\n",
      "[neptune] [warning] NeptuneWarning: By default, these monitoring options are disabled in interactive sessions: 'capture_stdout', 'capture_stderr', 'capture_traceback', 'capture_hardware_metrics'. You can set them to 'True' when initializing the run and the monitoring will continue until you call run.stop() or the kernel stops. NOTE: To track the source files, pass their paths to the 'source_code' argument. For help, see: https://docs-legacy.neptune.ai/logging/source_code/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 18 posts from 2025-07-07 to 2025-07-08\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/krishnadasm/wallstreetbets-scraper/e/WAL-73\n",
      "Saved 38 posts to reddit_posts_20250709_172911.csv\n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] Waiting for the remaining 17 operations to synchronize with Neptune. Do not kill this process.\n",
      "[neptune] [info   ] All 17 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/krishnadasm/wallstreetbets-scraper/e/WAL-73/metadata\n",
      "Choose an example to run by uncommenting the appropriate line!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Pushshift-Style Reddit Scraper\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Uncomment the example you want to run:\n",
    "    \n",
    "    # Example 1: Date range scraping\n",
    "    # example_date_range_scraping()\n",
    "    \n",
    "    # Example 2: Historical batch scraping  \n",
    "    example_historical_scraping()\n",
    "    \n",
    "    # Example 3: Keyword search\n",
    "    # example_keyword_search()\n",
    "    \n",
    "    print(\"Choose an example to run by uncommenting the appropriate line!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
