{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bc29706",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf374148",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import neptune\n",
    "import praw\n",
    "from datetime import datetime\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "Root = Path('.').absolute().parent\n",
    "SCRIPTS = Root / r'scripts'\n",
    "# SCRIPTS = Root / r'C:\\Users\\Admin\\Projects\\ML Projects\\ManipDetect\\research\\scripts'\n",
    "DATA = Root/ r'C:\\Users\\krishnadas\\Projects\\ML Projects\\ManipDetect\\data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f735fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many posts are there\n",
    "# %pip install psaw\n",
    "################# Too slow #################\n",
    "# from psaw import PushshiftAPI\n",
    "# import pandas as pd\n",
    "\n",
    "# api = PushshiftAPI()\n",
    "# subreddit_name = \"Wallstreetbetsnew\"  # Replace with the desired subreddit\n",
    "# api_request_generator = api.search_submissions(subreddit=subreddit_name, score=\">=0\")\n",
    "# submissions = pd.DataFrame([submission for submission in api_request_generator])\n",
    "# total_posts = len(submissions)\n",
    "# print(f\"Total posts in r/{subreddit_name}: {total_posts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540a2afb",
   "metadata": {},
   "source": [
    "### Reddit connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "131f47f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reddit_connect():\n",
    "    \"\"\"Initialize Reddit connection\"\"\"\n",
    "    load_dotenv()\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=os.getenv('REDDIT_CLIENT_ID'),\n",
    "        client_secret=os.getenv('REDDIT_CLIENT_SECRET'),\n",
    "        user_agent=os.getenv('REDDIT_USER_AGENT'),\n",
    "        username=os.getenv('REDDIT_USERNAME'),\n",
    "        password=os.getenv('REDDIT_PASSWORD')\n",
    "    )\n",
    "    return reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff1285e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post ID: 1ltjdp7\n",
      "Title: BMNR & RGC Explode! üî• Massive Moves You Can‚Äôt Miss!\n",
      "Text: \n",
      "üöÄ $BMNR and $RGC are blasting off ‚Äì and this video breaks it all down! We dive into the jaw-dropping price spikes, the catalysts behind the runs, and what to watch next. Whether you're riding the momentum or just curious, we cover:\n",
      "\n",
      "üìà BMNR‚Äôs staggering % gains\n",
      "üîç What‚Äôs fueling the surge in RGC\n",
      "üéØ Key resistance levels & entry zones\n",
      "üîî Risk tips & what could flip the script\n",
      "\n",
      "üëâ Hit LIKE if you're hyped, COMMENT with your take, and SUBSCRIBE for daily market breakdowns!\n",
      "\n",
      "https://youtu.be/43-Z2T3nLlI?si=xg_F0B4iT-ayqRl1\n",
      "\n",
      "\n",
      "Author: Mino3621\n",
      "author id: rqvik0bj\n",
      "Created: 2025-07-07 04:58:19\n",
      "Created UTC: 1751857099.0\n",
      "URL: https://www.reddit.com/r/Wallstreetbetsnew/comments/1ltjdp7/bmnr_rgc_explode_massive_moves_you_cant_miss/\n",
      "Score: 2\n",
      "Comments: 1\n",
      "----------------------------------------\n",
      "Post ID: 1ltefwz\n",
      "Title: Two Special Situations with Near Term Catalysts: $DATS and $HIT;\n",
      "Two After Hours Market Movers: $MDIA, $CGTX\n",
      "Text: **After Hours Market Movers--$MDIA, $CGTX--on watch for potential follow through in pre-market Monday.**\n",
      "\n",
      "**DatChat¬† $DATS $2.62** may be one of those Special Situations that has not been recognized yet by the market-at-large. First, DATS has developed a patented technology which offers users a convenient **Photo Sharing app with User Interfaces (UI) which users can control who views those photos posted in the cloud.** When this app is launched in the next sixty days (as per the CEO in a recent public interview), the potential for the app going explosively viral is very high because users have to set up a digital photo album to be shared with those who have been invited to join that \"album\". **Yes--very unlike Facebook/Meta (FB) in a good way.**\n",
      "\n",
      "Secondly, DATS owns 34% of a pending IPO of RPM Interactive.¬† that is anticipated--again-- in the next sixty days.¬† **Press Release**:¬† [https://finance.yahoo.com/news/datchat-subsidiary-rpm-interactive-files-121500230.html](https://finance.yahoo.com/news/datchat-subsidiary-rpm-interactive-files-121500230.html)¬† DATS Current Market Cap is only $12.4 Million--so that low market cap clearly is not taking into account the value of the IPO shares\\*\\*.¬† A near term catalyst will be the Shareholder of Record Date that will be announced.¬† It would not be a surprise that traders will buy in before the shareholder of record date to participate in the spin out RPM Interactive. ¬† DATS will still be the sole owner of Myseum (the Photo Sharing App).\n",
      "\n",
      "**Health in Technology $HIT $0.85** This look like another Special Situation/Opportunity. After its IPO in December and running to over $7.50, the stock sold off on massive volume in one (1) day to $1.25 with no news,no new SEC filing no bad press. Don't take my word for it. See the chart-- --[https://stockcharts.com/sc3/ui/?s=HIT¬†¬†](https://stockcharts.com/sc3/ui/?s=HIT%C2%A0%C2%A0) **The company's Cash position is $7.6 million as of March 31, 2025) with the market cap is $47 million, which considering that the company's revenues are growing rapidly--Quarter to Quarter--and reporting Net Income increasing over 257% over the First Quarter, 2024, it looks undervalued.**¬† Revenues increased 56% vs¬† First Quarter, 2024 to $8 Million. HIT CEO Interview was posted two weeks ago--https://www.youtube.com/watch?v=q8KsYEY7ZOQ&t=2039s¬† Corporate Description: HIT's Insurtech platform company, backed by third-party AI technology, offers a marketplace to improve processes in the healthcare industry through vertical integration, process simplification, and automation. By removing friction and complexities, HIT streamlines the underwriting, sales and service process for insurance companies, licensed brokers. Recent Research Report has $2.50 Target Price.\n",
      "\n",
      "*Due diligence is always necessary to make informed investment decisions. So take these ideas as a starting point and not your entire due diligence effort.*\n",
      "Author: Marketspike\n",
      "author id: a159k26r\n",
      "Created: 2025-07-07 00:49:47\n",
      "Created UTC: 1751842187.0\n",
      "URL: https://www.reddit.com/r/Wallstreetbetsnew/comments/1ltefwz/two_special_situations_with_near_term_catalysts/\n",
      "Score: 3\n",
      "Comments: 0\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test scraping with a single post without function\n",
    "reddit = reddit_connect()\n",
    "subreddit = reddit.subreddit(\"wallstreetbetsnew\")\n",
    "posts = subreddit.new(limit=2)\n",
    "for post in posts:\n",
    "    print(f\"Post ID: {post.id}\")\n",
    "    print(f\"Title: {post.title}\")\n",
    "    print(f\"Text: {post.selftext if post.selftext else 'N/A'}\")\n",
    "    print(f\"Author: {post.author.name if post.author else 'N/A'}\")\n",
    "    print(f'author id: {post.author.id if post.author else \"N/A\"}')\n",
    "    print(f\"Created: {datetime.fromtimestamp(post.created_utc)}\")\n",
    "    print(f'Created UTC: {post.created_utc}')\n",
    "    print(f\"URL: {post.url}\")\n",
    "    print(f\"Score: {post.score}\")\n",
    "    print(f\"Comments: {post.num_comments}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c57f5c",
   "metadata": {},
   "source": [
    "### Build data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "827c2d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_progress(filename=\"scraping_progress.json\"):\n",
    "    \"\"\"Load previously scraped data if it exists\"\"\"\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            posts = data.get('posts', [])\n",
    "            \n",
    "            # Normalize old data structure to ensure consistency\n",
    "            consistent_posts = []\n",
    "            for post in posts:\n",
    "                # Ensure all required fields exist with default values\n",
    "                consistent_post_dict = {\n",
    "                    'post_id': post.get('post_id', ''),\n",
    "                    'title': post.get('title', ''),\n",
    "                    'text': post.get('text', post.get('selftext', '')),  # Handle old 'selftext' field\n",
    "                    'post_type': post.get('post_type', 'unknown'),\n",
    "                    'author_name': post.get('author_name', post.get('author', '[unknown]')),  # Handle old 'author' field\n",
    "                    'author_id':post.get('author_id', post.get('author_id', '')),\n",
    "                    'score': post.get('score', 0),\n",
    "                    'num_comments': post.get('num_comments', 0),\n",
    "                    'created_utc': post.get('created_utc', 0),\n",
    "                    'url': post.get('url', '')\n",
    "                }\n",
    "                consistent_posts.append(consistent_post_dict)\n",
    "            \n",
    "            return consistent_posts, data.get('last_post_id', None)\n",
    "    except FileNotFoundError:\n",
    "        return [], None\n",
    "\n",
    "def save_progress(posts_data, last_post_id, filename=\"scraping_progress.json\"):\n",
    "    \"\"\"Save current progress to file\"\"\"\n",
    "    progress_data = {\n",
    "        'posts': posts_data,\n",
    "        'last_post_id': last_post_id,\n",
    "        'saved_at': datetime.now().isoformat(),\n",
    "        'total_posts': len(posts_data)\n",
    "    }\n",
    "    # Save progress as JSON (for resume functionality)\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(progress_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # Also save current data as CSV\n",
    "    if posts_data:\n",
    "        df = pd.DataFrame(posts_data)\n",
    "        df.to_csv(\"wallstreetbetsnew_posts.csv\", index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "519c2526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_final_csv(posts_data, filepath):\n",
    "    \"\"\"Save final dataset as CSV with proper formatting\"\"\"\n",
    "    if not posts_data:\n",
    "        return\n",
    "        \n",
    "    df = pd.DataFrame(posts_data)\n",
    "    \n",
    "    # Convert timestamp to readable format\n",
    "    df['created_datetime'] = pd.to_datetime(df['created_utc'], unit='s')\n",
    "    \n",
    "    # Reorder columns for better readability, but only use columns that exist\n",
    "    preferred_columns_order = ['post_id', 'title', 'text', 'post_type', 'author_name', 'author_id', 'score', 'num_comments', \n",
    "                                'created_utc', 'url']\n",
    "    \n",
    "    # Filter to only include columns that actually exist in the DataFrame\n",
    "    available_columns = [col for col in preferred_columns_order if col in df.columns]\n",
    "    \n",
    "    # Add any remaining columns that weren't in our preferred order\n",
    "    remaining_columns = [col for col in df.columns if col not in available_columns]\n",
    "    final_columns_order = available_columns + remaining_columns\n",
    "    \n",
    "    df = df[final_columns_order]\n",
    "    \n",
    "    # Save with timestamp in filename\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    csv_filename = filepath/f\"wallstreetbetsnew_posts_{timestamp}.csv\"\n",
    "    df.to_csv(csv_filename, index=False, encoding='utf-8')\n",
    "    \n",
    "    return csv_filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea8036d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(reddit, target_posts=10):\n",
    "    \"\"\"Build the dataset by scraping WallStreetBetsnew posts\"\"\"\n",
    "    \n",
    "    # Load previous progress\n",
    "    filepath = r'C:\\Users\\krishnadas\\Projects\\ML Projects\\ManipDetect\\research\\scripts\\temp_data'  # Define the path to save progress\n",
    "    posts_data, last_post_id = load_progress()\n",
    "    start_count = len(posts_data)\n",
    "    \n",
    "    if start_count > 0:\n",
    "        print(f\"Resuming from {start_count} previously scraped posts...\")\n",
    "    else:\n",
    "        print(\"Starting fresh scrape...\")\n",
    "    \n",
    "    # Get subreddit\n",
    "    # subreddit = reddit.subreddit(\"wallstreetbets\")\n",
    "    # change to new subreddit\n",
    "    subreddit = reddit.subreddit(\"wallstreetbetsnew\")\n",
    "\n",
    "    \n",
    "    # Track scraping metrics\n",
    "    start_time = time.time()\n",
    "    errors_count = 0\n",
    "    \n",
    "    try:\n",
    "        # Get posts (PRAW handles pagination automatically)\n",
    "        posts_generator = subreddit.new(limit=target_posts)\n",
    "        \n",
    "        # Convert to list to get total count for progress bar\n",
    "        print(\"Fetching post list from Reddit...\")\n",
    "        all_posts = list(posts_generator)\n",
    "        \n",
    "        # Skip posts we already have if resuming\n",
    "        if last_post_id:\n",
    "            # Find where to resume\n",
    "            resume_index = 0\n",
    "            for i, post in enumerate(all_posts):\n",
    "                if post.id == last_post_id:\n",
    "                    resume_index = i + 1\n",
    "                    break\n",
    "            all_posts = all_posts[resume_index:]\n",
    "            print(f\"Resuming from post index {resume_index}\")\n",
    "        \n",
    "        # Process remaining posts\n",
    "        posts_to_process = min(len(all_posts), target_posts - start_count)\n",
    "        \n",
    "        for i, submission in enumerate(tqdm(all_posts[:posts_to_process], \n",
    "                                        desc=\"Scraping posts\", \n",
    "                                        initial=start_count, \n",
    "                                        total=target_posts)):\n",
    "            try:\n",
    "                # Handle author safely\n",
    "                author_name = \"[deleted]\"\n",
    "                author_id = None\n",
    "                if submission.author is not None:\n",
    "                    try:\n",
    "                        author_name = submission.author.name\n",
    "                        author_id = submission.author.id\n",
    "                    except Exception:\n",
    "                        author_name = \"[unavailable]\"\n",
    "                        author_id = None\n",
    "                \n",
    "                # Extract post text content with better categorization\n",
    "                post_text = \"\"\n",
    "                post_type = \"text\"  # Default type\n",
    "                \n",
    "                if submission.is_self:  # Text post\n",
    "                    if submission.selftext:\n",
    "                        post_text = submission.selftext\n",
    "                        post_type = \"text\"\n",
    "                    else:\n",
    "                        post_text = \"[Empty text post]\"\n",
    "                        post_type = \"text_empty\"\n",
    "                else:  # Link post\n",
    "                    post_text = \"[Link Post]\"\n",
    "                    post_type = \"link\"\n",
    "                    \n",
    "                    # You could also categorize by URL type\n",
    "                    if any(img_ext in submission.url.lower() for img_ext in ['.jpg', '.jpeg', '.png', '.gif']):\n",
    "                        post_type = \"image\"\n",
    "                    elif 'youtube.com' in submission.url.lower() or 'youtu.be' in submission.url.lower():\n",
    "                        post_type = \"video\"\n",
    "                \n",
    "                # Extract post data (keeping titles and emojis intact)\n",
    "                post_info = {\n",
    "                    \"post_id\": submission.id,\n",
    "                    \"title\": submission.title,  # Preserves emojis and formatting\n",
    "                    \"text\": post_text,  # The actual post content\n",
    "                    \"post_type\": post_type,  # Type of post for analysis\n",
    "                    \"author_name\": author_name,  # Author's name\n",
    "                    \"author_id\": author_id,  # Author's ID\n",
    "                    \"score\": submission.score,\n",
    "                    \"created_utc\": submission.created_utc,\n",
    "                    \"num_comments\": submission.num_comments,\n",
    "                    \"url\": submission.url,\n",
    "                }\n",
    "                \n",
    "                posts_data.append(post_info)\n",
    "                \n",
    "                # Save progress every 50 posts\n",
    "                if (len(posts_data) - start_count) % 50 == 0:\n",
    "                    save_progress(posts_data, submission.id)\n",
    "                \n",
    "                # Small delay every 100 posts to be nice to Reddit\n",
    "                if (len(posts_data) - start_count) % 100 == 0:\n",
    "                    time.sleep(1)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                errors_count += 1\n",
    "                print(f\"Error processing post {submission.id}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Final save\n",
    "        save_progress(posts_data, posts_data[-1][\"post_id\"] if posts_data else None)\n",
    "        \n",
    "        # Calculate final metrics\n",
    "        end_time = time.time()\n",
    "        scraping_duration = end_time - start_time\n",
    "        \n",
    "        # Save final CSV dataset\n",
    "        csv_filename = save_final_csv(posts_data, filepath)\n",
    "        \n",
    "        # Return metrics for tracking\n",
    "        metrics = {\n",
    "            'total_posts_collected': len(posts_data),\n",
    "            'new_posts_this_session': len(posts_data) - start_count,\n",
    "            'scraping_duration_minutes': scraping_duration / 60,\n",
    "            'errors_encountered': errors_count,\n",
    "            'posts_per_minute': (len(posts_data) - start_count) / (scraping_duration / 60) if scraping_duration > 0 else 0,\n",
    "            'csv_filename': str(csv_filename),\n",
    "            'total_posts_available': len(all_posts),\n",
    "            'resumed_from': start_count if start_count > 0 else None\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nDataset building completed!\")\n",
    "        print(f\"Total posts collected: {metrics['total_posts_collected']}\")\n",
    "        print(f\"New posts this session: {metrics['new_posts_this_session']}\")\n",
    "        print(f\"Duration: {metrics['scraping_duration_minutes']:.2f} minutes\")\n",
    "        print(f\"Errors: {metrics['errors_encountered']}\")\n",
    "        \n",
    "        return posts_data, metrics\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nScraping interrupted by user. Progress saved.\")\n",
    "        save_progress(posts_data, posts_data[-1][\"post_id\"] if posts_data else None)\n",
    "        return posts_data, {'interrupted': True, 'posts_at_interruption': len(posts_data)}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {str(e)}\")\n",
    "        save_progress(posts_data, posts_data[-1][\"post_id\"] if posts_data else None)\n",
    "        return posts_data, {'fatal_error': str(e), 'posts_at_error': len(posts_data)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9172d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def track_scraping_metrics(posts_data, metrics, target_posts):\n",
    "    \"\"\"Track scraping metrics and results in Neptune\"\"\"\n",
    "    # Initialize Neptune\n",
    "    run = neptune.init_run(project=\"krishnadasm/wallstreetbets-scraper\")\n",
    "    \n",
    "    # Log configuration\n",
    "    run[\"config/target_posts\"] = target_posts\n",
    "    run[\"config/subreddit\"] = \"wallstreetbetsnew\"\n",
    "    run[\"config/sort_method\"] = \"new\"\n",
    "    run[\"config/resume_enabled\"] = True\n",
    "    \n",
    "    # Log scraping metrics\n",
    "    if 'interrupted' in metrics:\n",
    "        run[\"scraping/interrupted\"] = True\n",
    "        run[\"scraping/posts_at_interruption\"] = metrics['posts_at_interruption']\n",
    "    elif 'fatal_error' in metrics:\n",
    "        run[\"scraping/fatal_error\"] = metrics['fatal_error']\n",
    "        run[\"scraping/posts_at_error\"] = metrics['posts_at_error']\n",
    "    else:\n",
    "        # Log successful completion metrics\n",
    "        run[\"results/total_posts_collected\"] = metrics['total_posts_collected']\n",
    "        run[\"results/new_posts_this_session\"] = metrics['new_posts_this_session']\n",
    "        run[\"results/scraping_duration_minutes\"] = metrics['scraping_duration_minutes']\n",
    "        run[\"results/errors_encountered\"] = metrics['errors_encountered']\n",
    "        run[\"results/posts_per_minute\"] = metrics['posts_per_minute']\n",
    "        run[\"scraping/total_posts_available\"] = metrics['total_posts_available']\n",
    "        \n",
    "        if metrics['resumed_from']:\n",
    "            run[\"scraping/resumed_from\"] = metrics['resumed_from']\n",
    "        \n",
    "        # Upload final dataset to Neptune\n",
    "        if metrics.get('csv_filename'):\n",
    "            run[\"data/posts_dataset\"].upload(metrics['csv_filename'])\n",
    "        run[\"data/progress_file\"].upload(\"scraping_progress.json\")\n",
    "    \n",
    "    # Log final progress\n",
    "    run[\"scraping/final_progress\"] = len(posts_data)\n",
    "    \n",
    "    run.stop()\n",
    "    print(\"Metrics logged to Neptune successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "06cc328f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Reddit API successfully.\n",
      "Resuming from 10 previously scraped posts...\n",
      "Fetching post list from Reddit...\n",
      "Resuming from post index 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping posts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error: unsupported operand type(s) for /: 'str' and 'str'\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/krishnadasm/wallstreetbets-scraper/e/WAL-38\n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] Waiting for the remaining 13 operations to synchronize with Neptune. Do not kill this process.\n",
      "[neptune] [info   ] All 13 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/krishnadasm/wallstreetbets-scraper/e/WAL-38/metadata\n",
      "Metrics logged to Neptune successfully!\n"
     ]
    }
   ],
   "source": [
    "def scrape_wallstreetbetsnew():\n",
    "    \"\"\"Main function that orchestrates the scraping process\"\"\"\n",
    "    # Build the dataset\n",
    "    reddit = reddit_connect()\n",
    "    print(\"Connected to Reddit API successfully.\")\n",
    "    posts_data, metrics = build_dataset(reddit, target_posts=10)\n",
    "    \n",
    "    # Track metrics in Neptune\n",
    "    track_scraping_metrics(posts_data, metrics, target_posts=10)\n",
    "    \n",
    "    return posts_data, metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_wallstreetbetsnew()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3682b332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "post_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "post_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "author_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "author_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "score",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "num_comments",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "created_utc",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "url",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "13428069-b45d-4b11-99b5-ce11dc3ba15a",
       "rows": [
        [
         "0",
         "1ltjdp7",
         "BMNR & RGC Explode! üî• Massive Moves You Can‚Äôt Miss!",
         "\nüöÄ $BMNR and $RGC are blasting off ‚Äì and this video breaks it all down! We dive into the jaw-dropping price spikes, the catalysts behind the runs, and what to watch next. Whether you're riding the momentum or just curious, we cover:\n\nüìà BMNR‚Äôs staggering % gains\nüîç What‚Äôs fueling the surge in RGC\nüéØ Key resistance levels & entry zones\nüîî Risk tips & what could flip the script\n\nüëâ Hit LIKE if you're hyped, COMMENT with your take, and SUBSCRIBE for daily market breakdowns!\n\nhttps://youtu.be/43-Z2T3nLlI?si=xg_F0B4iT-ayqRl1\n\n",
         "text",
         "Mino3621",
         "rqvik0bj",
         "2",
         "1",
         "1751857099.0",
         "https://www.reddit.com/r/Wallstreetbetsnew/comments/1ltjdp7/bmnr_rgc_explode_massive_moves_you_cant_miss/"
        ],
        [
         "1",
         "1ltefwz",
         "Two Special Situations with Near Term Catalysts: $DATS and $HIT;\nTwo After Hours Market Movers: $MDIA, $CGTX",
         "**After Hours Market Movers--$MDIA, $CGTX--on watch for potential follow through in pre-market Monday.**\n\n**DatChat¬† $DATS $2.62** may be one of those Special Situations that has not been recognized yet by the market-at-large. First, DATS has developed a patented technology which offers users a convenient **Photo Sharing app with User Interfaces (UI) which users can control who views those photos posted in the cloud.** When this app is launched in the next sixty days (as per the CEO in a recent public interview), the potential for the app going explosively viral is very high because users have to set up a digital photo album to be shared with those who have been invited to join that \"album\". **Yes--very unlike Facebook/Meta (FB) in a good way.**\n\nSecondly, DATS owns 34% of a pending IPO of RPM Interactive.¬† that is anticipated--again-- in the next sixty days.¬† **Press Release**:¬† [https://finance.yahoo.com/news/datchat-subsidiary-rpm-interactive-files-121500230.html](https://finance.yahoo.com/news/datchat-subsidiary-rpm-interactive-files-121500230.html)¬† DATS Current Market Cap is only $12.4 Million--so that low market cap clearly is not taking into account the value of the IPO shares\\*\\*.¬† A near term catalyst will be the Shareholder of Record Date that will be announced.¬† It would not be a surprise that traders will buy in before the shareholder of record date to participate in the spin out RPM Interactive. ¬† DATS will still be the sole owner of Myseum (the Photo Sharing App).\n\n**Health in Technology $HIT $0.85** This look like another Special Situation/Opportunity. After its IPO in December and running to over $7.50, the stock sold off on massive volume in one (1) day to $1.25 with no news,no new SEC filing no bad press. Don't take my word for it. See the chart-- --[https://stockcharts.com/sc3/ui/?s=HIT¬†¬†](https://stockcharts.com/sc3/ui/?s=HIT%C2%A0%C2%A0) **The company's Cash position is $7.6 million as of March 31, 2025) with the market cap is $47 million, which considering that the company's revenues are growing rapidly--Quarter to Quarter--and reporting Net Income increasing over 257% over the First Quarter, 2024, it looks undervalued.**¬† Revenues increased 56% vs¬† First Quarter, 2024 to $8 Million. HIT CEO Interview was posted two weeks ago--https://www.youtube.com/watch?v=q8KsYEY7ZOQ&t=2039s¬† Corporate Description: HIT's Insurtech platform company, backed by third-party AI technology, offers a marketplace to improve processes in the healthcare industry through vertical integration, process simplification, and automation. By removing friction and complexities, HIT streamlines the underwriting, sales and service process for insurance companies, licensed brokers. Recent Research Report has $2.50 Target Price.\n\n*Due diligence is always necessary to make informed investment decisions. So take these ideas as a starting point and not your entire due diligence effort.*",
         "text",
         "Marketspike",
         "a159k26r",
         "3",
         "0",
         "1751842187.0",
         "https://www.reddit.com/r/Wallstreetbetsnew/comments/1ltefwz/two_special_situations_with_near_term_catalysts/"
        ],
        [
         "2",
         "1ltcd94",
         "Investment Opportunity of a Lifetime - $215M Mid Cap about to Announce 2 Trial Results each worth 20X the Current Market Value.",
         "[https://stocktwits.com/Gps\\_100X\\_ROI\\_Potential/message/619753316](https://stocktwits.com/Gps_100X_ROI_Potential/message/619753316) \n\n\n\n[$SLS](https://stocktwits.com/symbol/SLS) will SLS be worth a few pennies more or less tomorrow? \n\nIDRK, but I do know, SLS will absoFreakinLutely be worth MANY DOLLARS MORE \n\n\\- ANY DAY NOW \n\n\\- When We Get and the Whole Market Sees the FDA Registrational Phase 3 Trial Results for Gps Immunotherapy and the SLS009/ Tambiciclib FDA Accelerated Approval Announcement.    \n   \nGps and SLS009/TambiCiclib are both worth Many Billions, and likely +$10B each to Big Pharma - the \"market pricing\", is just beginning to appreciate and reflect that value.   \n   \nNo Real Investors currently Holding are Letting any Go - with the FDA Registrational Results getting Announced Any Day Now.  \n\n  \nYou will Kick yourself For Not ADDING AS MANY AS YOU CAN... \n\n  \nFOR Those of US ALL IN, BALL DEEP - Congrats - this is about double a couple times, even withOUT News, the potential Value is Ginormous compared to the short rigged mcap.   \n\\- YOLO -- Degen Mode on [$SLS](https://stocktwits.com/symbol/SLS) Right Now for Massive ROI.   \n\\- mark this post.\n\nhttps://preview.redd.it/scqfhjtwlbbf1.jpg?width=1320&format=pjpg&auto=webp&s=c3ea4367cbcb7633201b3d62f8d9e3d410b0cc57\n\nhttps://preview.redd.it/hbtzqgtwlbbf1.jpg?width=1308&format=pjpg&auto=webp&s=8c1b5275eebaf661032379fbcb76b55f535353c8\n\n",
         "text",
         "Run4theRoses2",
         "60p5ejtz",
         "5",
         "0",
         "1751836724.0",
         "https://www.reddit.com/r/Wallstreetbetsnew/comments/1ltcd94/investment_opportunity_of_a_lifetime_215m_mid_cap/"
        ],
        [
         "3",
         "1lsi2f1",
         "AGMH Maby look at this?",
         "\n\nAGMH signed a deal to sell a subsidiary for $57M, with the first payment (20%) due by July 10. The company has strengthened its board and increased the CEO‚Äôs voting power‚Äîoften a sign that a major event is nearing. Market cap is still under $5M (!). If the deal closes, $25‚Äì30 per share could be justified based on cash value alone.\nAGMH signed a deal to sell a subsidiary for $57M, with the first payment (20%) due by July 10. The company has strengthened its board and increased the CEO‚Äôs voting power‚Äîoften a sign that a major event is nearing. Market cap is still under $5M (!). If the deal closes, $25‚Äì30 per share could be justified based on cash value alone.",
         "text",
         "mariusvell",
         "nspvrhts",
         "30",
         "0",
         "1751744023.0",
         "https://www.reddit.com/r/Wallstreetbetsnew/comments/1lsi2f1/agmh_maby_look_at_this/"
        ],
        [
         "4",
         "1lsacg0",
         "Why invest in handsome?",
         "So why invest in handsome‚Ä¶ in beautiful‚Ä¶ in brave?\n\nWarren Buffett said he would never bet against America‚Ä¶ Why?  \n\nLet‚Äôs start with the fact that it is called beautiful‚Ä¶ and a promise of a home for those who are brave‚Ä¶\n\nThose who struggle through the darkness‚Ä¶ to make the world a new‚Ä¶ the better angels who create a kinder world for all walks of life no matter where they started‚Ä¶ anything is possible‚Ä¶ if you don‚Äôt give up‚Ä¶ \n\nIn a world economy of rum‚Ä¶ sugar‚Ä¶ and tea‚Ä¶ built with a force function of slavery‚Ä¶ it wasn‚Äôt inevitable it had to stay in the darkness‚Ä¶ it could change.  \n\nAlthough we could avoid a violent revolution followed by a civil war this time around‚Ä¶ \n\nA modern economy with a force function of AI could be built around the light of our best selves‚Ä¶ a universal access to basic food and housing‚Ä¶ to income‚Ä¶ and like in the parable of talents‚Ä¶ some could invest it‚Ä¶ in a new triangle trade of technology‚Ä¶ real estate‚Ä¶ and MainStreet‚Ä¶\n\nor\n\nNone of that will happen‚Ä¶ \n\nBut the fact that you thought it could‚Ä¶ that means you‚Äôre brave‚Ä¶.  \n\n",
         "text",
         "Future_Fund2025",
         "1ivpzm4bpm",
         "0",
         "2",
         "1751723437.0",
         "https://www.reddit.com/r/Wallstreetbetsnew/comments/1lsacg0/why_invest_in_handsome/"
        ]
       ],
       "shape": {
        "columns": 10,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>post_type</th>\n",
       "      <th>author_name</th>\n",
       "      <th>author_id</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1ltjdp7</td>\n",
       "      <td>BMNR &amp; RGC Explode! üî• Massive Moves You Can‚Äôt ...</td>\n",
       "      <td>\\nüöÄ $BMNR and $RGC are blasting off ‚Äì and this...</td>\n",
       "      <td>text</td>\n",
       "      <td>Mino3621</td>\n",
       "      <td>rqvik0bj</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.751857e+09</td>\n",
       "      <td>https://www.reddit.com/r/Wallstreetbetsnew/com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1ltefwz</td>\n",
       "      <td>Two Special Situations with Near Term Catalyst...</td>\n",
       "      <td>**After Hours Market Movers--$MDIA, $CGTX--on ...</td>\n",
       "      <td>text</td>\n",
       "      <td>Marketspike</td>\n",
       "      <td>a159k26r</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.751842e+09</td>\n",
       "      <td>https://www.reddit.com/r/Wallstreetbetsnew/com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1ltcd94</td>\n",
       "      <td>Investment Opportunity of a Lifetime - $215M M...</td>\n",
       "      <td>[https://stocktwits.com/Gps\\_100X\\_ROI\\_Potent...</td>\n",
       "      <td>text</td>\n",
       "      <td>Run4theRoses2</td>\n",
       "      <td>60p5ejtz</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.751837e+09</td>\n",
       "      <td>https://www.reddit.com/r/Wallstreetbetsnew/com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1lsi2f1</td>\n",
       "      <td>AGMH Maby look at this?</td>\n",
       "      <td>\\n\\nAGMH signed a deal to sell a subsidiary fo...</td>\n",
       "      <td>text</td>\n",
       "      <td>mariusvell</td>\n",
       "      <td>nspvrhts</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>1.751744e+09</td>\n",
       "      <td>https://www.reddit.com/r/Wallstreetbetsnew/com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1lsacg0</td>\n",
       "      <td>Why invest in handsome?</td>\n",
       "      <td>So why invest in handsome‚Ä¶ in beautiful‚Ä¶ in br...</td>\n",
       "      <td>text</td>\n",
       "      <td>Future_Fund2025</td>\n",
       "      <td>1ivpzm4bpm</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.751723e+09</td>\n",
       "      <td>https://www.reddit.com/r/Wallstreetbetsnew/com...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id                                              title  \\\n",
       "0  1ltjdp7  BMNR & RGC Explode! üî• Massive Moves You Can‚Äôt ...   \n",
       "1  1ltefwz  Two Special Situations with Near Term Catalyst...   \n",
       "2  1ltcd94  Investment Opportunity of a Lifetime - $215M M...   \n",
       "3  1lsi2f1                            AGMH Maby look at this?   \n",
       "4  1lsacg0                            Why invest in handsome?   \n",
       "\n",
       "                                                text post_type  \\\n",
       "0  \\nüöÄ $BMNR and $RGC are blasting off ‚Äì and this...      text   \n",
       "1  **After Hours Market Movers--$MDIA, $CGTX--on ...      text   \n",
       "2  [https://stocktwits.com/Gps\\_100X\\_ROI\\_Potent...      text   \n",
       "3  \\n\\nAGMH signed a deal to sell a subsidiary fo...      text   \n",
       "4  So why invest in handsome‚Ä¶ in beautiful‚Ä¶ in br...      text   \n",
       "\n",
       "       author_name   author_id  score  num_comments   created_utc  \\\n",
       "0         Mino3621    rqvik0bj      2             1  1.751857e+09   \n",
       "1      Marketspike    a159k26r      3             0  1.751842e+09   \n",
       "2    Run4theRoses2    60p5ejtz      5             0  1.751837e+09   \n",
       "3       mariusvell    nspvrhts     30             0  1.751744e+09   \n",
       "4  Future_Fund2025  1ivpzm4bpm      0             2  1.751723e+09   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.reddit.com/r/Wallstreetbetsnew/com...  \n",
       "1  https://www.reddit.com/r/Wallstreetbetsnew/com...  \n",
       "2  https://www.reddit.com/r/Wallstreetbetsnew/com...  \n",
       "3  https://www.reddit.com/r/Wallstreetbetsnew/com...  \n",
       "4  https://www.reddit.com/r/Wallstreetbetsnew/com...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"wallstreetbetsnew_posts.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc60ccf3",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0bf4d976",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('scraping_progress.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "    posts = data.get('posts',[])\n",
    "    consistent_posts = []\n",
    "    for post in posts:\n",
    "        consistent_post_dict = {\n",
    "        'post_id': post.get('post_id', ''),\n",
    "        'title': post.get('title', ''),\n",
    "        'text': post.get('text', post.get('selftext', '')),  # Handle old 'selftext' field\n",
    "        'post_type': post.get('post_type', 'unknown'),\n",
    "        'author_name': post.get('author_name', post.get('author', '[unknown]')),  # Handle old 'author' field\n",
    "        'author_id': post.get('author_id', post.get('author_id', '')),\n",
    "        'score': post.get('score', 0),\n",
    "        'num_comments': post.get('num_comments', 0),\n",
    "        'created_utc': post.get('created_utc', 0),\n",
    "        'url': post.get('url', '')\n",
    "        }\n",
    "        consistent_posts.append(consistent_post_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6554b32a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1lsi2f1'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consistent_posts[3].get('post_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "92316282",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=list(set([post.get('post_id') for post in consistent_posts]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd044f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "a = [1, 2,3,4,5,6,7,9,0]\n",
    "a[:2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
